import torch
from torch import nn
from torch.nn import functional as F
from typing import Optional, Dict, Any
from .transformer_blocks import PareBasicTransformerBlock

class PareTransformer2DModel(nn.Module):
	def __init__(
		self,
		num_attention_heads: int = 16,
		attention_head_dim: int = 88,
		in_channels: Optional[int] = None,
		out_channels: Optional[int] = None,
		num_layers: int = 1,
		dropout: float = 0.0,
		norm_num_groups: int = 32,
		cross_attention_dim: Optional[int] = None,
		attention_bias: bool = False,
		num_vector_embeds: Optional[int] = None,
		patch_size: Optional[int] = None,
		use_linear_projection: bool = False,
		only_cross_attention: bool = False,
		double_self_attention: bool = False,
		upcast_attention: bool = False,
		norm_type: str = "layer_norm",
		norm_elementwise_affine: bool = True,
		norm_eps: float = 1e-5,
	):
		super().__init__()
		self.use_linear_projection = use_linear_projection
		self.num_attention_heads = num_attention_heads
		self.attention_head_dim = attention_head_dim
		inner_dim = num_attention_heads * attention_head_dim

		conv_cls = nn.Conv2d
		linear_cls = nn.Linear

		# 1. Transformer2DModel can process both standard continuous images of shape `(batch_size, num_channels, width, height)` as well as quantized image embeddings of shape `(batch_size, num_image_vectors)`
		# Define whether input is continuous or discrete depending on configuration
		self.is_input_continuous = (in_channels is not None) and (patch_size is None)
		self.is_input_vectorized = num_vector_embeds is not None
		self.is_input_patches = in_channels is not None and patch_size is not None

		# 2. Define input layers
		self.in_channels = in_channels

		self.norm = torch.nn.GroupNorm(num_groups=norm_num_groups, num_channels=in_channels, eps=1e-6, affine=True)
		if use_linear_projection:
			self.proj_in = linear_cls(in_channels, inner_dim)
		else:
			self.proj_in = conv_cls(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)

		# 3. Define transformers blocks
		self.transformer_blocks = nn.ModuleList(
			[
				PareBasicTransformerBlock(
					inner_dim,
					num_attention_heads,
					attention_head_dim,
					dropout=dropout,
					cross_attention_dim=cross_attention_dim,
					attention_bias=attention_bias,
					only_cross_attention=only_cross_attention,
					double_self_attention=double_self_attention,
					upcast_attention=upcast_attention,
					norm_type=norm_type,
					norm_elementwise_affine=norm_elementwise_affine,
					norm_eps=norm_eps,
				)
				for d in range(num_layers)
			]
		)

		# 4. Define output layers
		self.out_channels = in_channels if out_channels is None else out_channels
		if self.is_input_continuous:
			# TODO: should use out_channels for continuous projections
			if use_linear_projection:
				self.proj_out = linear_cls(inner_dim, in_channels)
			else:
				self.proj_out = conv_cls(inner_dim, in_channels, kernel_size=1, stride=1, padding=0)
		elif self.is_input_vectorized:
			self.norm_out = nn.LayerNorm(inner_dim)
			self.out = nn.Linear(inner_dim, self.num_vector_embeds - 1)
		elif self.is_input_patches and norm_type != "ada_norm_single":
			self.norm_out = nn.LayerNorm(inner_dim, elementwise_affine=False, eps=1e-6)
			self.proj_out_1 = nn.Linear(inner_dim, 2 * inner_dim)
			self.proj_out_2 = nn.Linear(inner_dim, patch_size * patch_size * self.out_channels)
		elif self.is_input_patches and norm_type == "ada_norm_single":
			self.norm_out = nn.LayerNorm(inner_dim, elementwise_affine=False, eps=1e-6)
			self.scale_shift_table = nn.Parameter(torch.randn(2, inner_dim) / inner_dim**0.5)
			self.proj_out = nn.Linear(inner_dim, patch_size * patch_size * self.out_channels)

		# 5. PixArt-Alpha blocks.
		self.adaln_single = None
		self.use_additional_conditions = False

		self.caption_projection = None
		self.gradient_checkpointing = False

	def _set_gradient_checkpointing(self, module, value=False):
		if hasattr(module, "gradient_checkpointing"):
			module.gradient_checkpointing = value

	def forward(
		self,
		hidden_states: torch.Tensor,
		encoder_hidden_states: Optional[torch.Tensor] = None,
		timestep: Optional[torch.LongTensor] = None,
		added_cond_kwargs: Dict[str, torch.Tensor] = None,
		class_labels: Optional[torch.LongTensor] = None,
		cross_attention_kwargs: Dict[str, Any] = None,
		attention_mask: Optional[torch.Tensor] = None,
		encoder_attention_mask: Optional[torch.Tensor] = None,
		return_dict: bool = True,
	):
		if attention_mask is not None and attention_mask.ndim == 2:
			attention_mask = (1 - attention_mask.to(hidden_states.dtype)) * -10000.0
			attention_mask = attention_mask.unsqueeze(1)

		# convert encoder_attention_mask to a bias the same way we do for attention_mask
		if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:
			encoder_attention_mask = (1 - encoder_attention_mask.to(hidden_states.dtype)) * -10000.0
			encoder_attention_mask = encoder_attention_mask.unsqueeze(1)
		
		# 1. Input
		if self.is_input_continuous:
			batch, _, height, width = hidden_states.shape
			residual = hidden_states

			hidden_states = self.norm(hidden_states)
			if not self.use_linear_projection:
				hidden_states = (
					self.proj_in(hidden_states)
				)
				inner_dim = hidden_states.shape[1]
				hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)
			else:
				inner_dim = hidden_states.shape[1]
				hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)
				hidden_states = (
					self.proj_in(hidden_states)
				)

		elif self.is_input_vectorized:
			hidden_states = self.latent_image_embedding(hidden_states)
		elif self.is_input_patches:
			height, width = hidden_states.shape[-2] // self.patch_size, hidden_states.shape[-1] // self.patch_size
			hidden_states = self.pos_embed(hidden_states)

			if self.adaln_single is not None:
				if self.use_additional_conditions and added_cond_kwargs is None:
					raise ValueError(
						"`added_cond_kwargs` cannot be None when using additional conditions for `adaln_single`."
					)
				batch_size = hidden_states.shape[0]
				timestep, embedded_timestep = self.adaln_single(
					timestep, added_cond_kwargs, batch_size=batch_size, hidden_dtype=hidden_states.dtype
				)

		# 2. Blocks
		if self.caption_projection is not None:
			batch_size = hidden_states.shape[0]
			encoder_hidden_states = self.caption_projection(encoder_hidden_states)
			encoder_hidden_states = encoder_hidden_states.view(batch_size, -1, hidden_states.shape[-1])

		for block in self.transformer_blocks:
			if self.training and self.gradient_checkpointing:

				def create_custom_forward(module, return_dict=None):
					def custom_forward(*inputs):
						if return_dict is not None:
							return module(*inputs, return_dict=return_dict)
						else:
							return module(*inputs)

					return custom_forward

				ckpt_kwargs: Dict[str, Any] = {"use_reentrant": False}
				hidden_states = torch.utils.checkpoint.checkpoint(
					create_custom_forward(block),
					hidden_states,
					attention_mask,
					encoder_hidden_states,
					encoder_attention_mask,
					timestep,
					cross_attention_kwargs,
					class_labels,
					**ckpt_kwargs,
				)
			else:
				hidden_states = block(
					hidden_states,
					attention_mask=attention_mask,
					encoder_hidden_states=encoder_hidden_states,
					encoder_attention_mask=encoder_attention_mask,
					timestep=timestep,
					cross_attention_kwargs=cross_attention_kwargs,
					class_labels=class_labels,
				)

		# 3. Output
		if self.is_input_continuous:
			if not self.use_linear_projection:
				hidden_states = hidden_states.reshape(batch, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()
				hidden_states = (
					self.proj_out(hidden_states)
				)
			else:
				hidden_states = (
					self.proj_out(hidden_states)
				)
				hidden_states = hidden_states.reshape(batch, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()

			output = hidden_states + residual
		elif self.is_input_vectorized:
			hidden_states = self.norm_out(hidden_states)
			logits = self.out(hidden_states)
			# (batch, self.num_vector_embeds - 1, self.num_latent_pixels)
			logits = logits.permute(0, 2, 1)

			# log(p(x_0))
			output = F.log_softmax(logits.double(), dim=1).float()

		if self.is_input_patches:
			if self.config.norm_type != "ada_norm_single":
				conditioning = self.transformer_blocks[0].norm1.emb(
					timestep, class_labels, hidden_dtype=hidden_states.dtype
				)
				shift, scale = self.proj_out_1(F.silu(conditioning)).chunk(2, dim=1)
				hidden_states = self.norm_out(hidden_states) * (1 + scale[:, None]) + shift[:, None]
				hidden_states = self.proj_out_2(hidden_states)
			elif self.config.norm_type == "ada_norm_single":
				shift, scale = (self.scale_shift_table[None] + embedded_timestep[:, None]).chunk(2, dim=1)
				hidden_states = self.norm_out(hidden_states)
				# Modulation
				hidden_states = hidden_states * (1 + scale) + shift
				hidden_states = self.proj_out(hidden_states)
				hidden_states = hidden_states.squeeze(1)

			# unpatchify
			if self.adaln_single is None:
				height = width = int(hidden_states.shape[1] ** 0.5)
			hidden_states = hidden_states.reshape(
				shape=(-1, height, width, self.patch_size, self.patch_size, self.out_channels)
			)
			hidden_states = torch.einsum("nhwpqc->nchpwq", hidden_states)
			output = hidden_states.reshape(
				shape=(-1, self.out_channels, height * self.patch_size, width * self.patch_size)
			)

		if not return_dict:
			return (output,)

		return output